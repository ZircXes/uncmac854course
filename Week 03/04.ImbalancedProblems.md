# Imbalanced and weighted classification problems
Oftentimes, the class we care about is not balanced 50/50 in the dataset of interest, and in some cases the downstream value model does not treat false positives, false negatives, true positives, and true negatives equally. In both of these cases, relying solely on the raw data and information value can lead to sub-optimal prediction performance.

#### Potential solutions to Imbalanced datasets
We generally have a few choices to evaluate classification models where there imbalance or the cost-function prefers sensitivity/specificity over accuracy. A few of these are provided here.

1. Re-balancing the dataset
   * Up-sampling the smaller class
   * Down-sampling the larger class
   * Auto re-balancing
   * Dollar re-balancing
2. Post-model threshold selection
   * Using ROC-curve with cost-function to optimize model
   * Using financial model for hyper-parameter search with validation set
3. Weighting class categories when fitting the model


### Fraud Case
Consider a fraud detection model where true fraud incidents are only 5% of the data. Without any model, we would have a choice of scanning all of the data for fraud or scanning none of the data. Our baseline accuracy by not evaluating any cases is 95%. Each fraud case costs the company a large amount of money, but evaluating every transaction is cost prohibitive. Consider the following facts:

```
Cost to review a transaction:     $     150
Monthly fraud losses:             $ 760,500
Average transaction size:         $     750
Transactions per month:              10,000
Fraud transactions per month:           500
Fraud % of transactions:                 5%
Parent Entropy                          29%
```

Baseline we would choose to do nothing as the cost to assess every transaction monthly would be $1.5 million, for a loss of $739 thousand monthly. This fraud loss is still problematic, and we would like to reduce this amount as much as possible.

In this case, accuracy is not a useful measure (our baseline accuracy is 95% by assuming no transaction is fraudulent).

#### Splitting on Transaction Size

Split on Transaction size (small vs. large).
```
                    Parent      Small       Large
Fraud                  500        350         150
Not Fraud            9,500      8,200       1,300
Total               10,000      8,550       1,450
Fraud %                 5%         4%         10%
Entropy              28.6%      24.7%       48.0%
Avg TX Size           $750        $30      $5,000
Total Fraud Loss  $760,500    $10,500    $750,000
% of Data             100%      85.5%       14.5%   

Weighted Average Entropy:                   28.0%   
Information Gain:                            0.6%
```

Evaluating only the large transactions segment, the net benefit is $532.5k. In this example, despite entropy increasing in that segment, the financial model prefers this split to become cost effective.

Net Benefit = `$5,000 * $150 - $150 * 1,450 = $750,000 - $217,500 = $532,500`

#### Other classification performance metrics

The problem with focusing on accuracy in this example is that predicting true positives is significantly more important than predicting true negatives. In this case, we have other options:

    Accuracy:                `(TP + TN) / (TP + TN + FP + FN)`

    Precision:               `TP / (TP + FP)`

    Sensitivity (recall):    `TP / (TP + FN)`

    Specificity:             `TN / (TN + FP)` 

    Net Revenue        

From our financial model, we can map net revenue to the confusion matrix and the TP, TN, FP, FN:

```
      Actual  Predict   Benefit      Cost      Net
TN         -        -         0         0        0
TP         +        +       750       150     +600
FP         -        +         0       150     -150
FN         +        -         0       750     -750
```

Note that depending on splits, the cost/benefits of TP and FN can adjust based on average transaction size, which makes this problem more complex. When we split on transaction size, the financial function for the large transaction node is:
```
      Actual  Predict   Benefit      Cost      Net
TN         -        -         0         0        0
TP         +        +      5000       150    +4850
FP         -        +         0       150     -150
FN         +        -         0      5000    -5000
```

In both scenarios, the cost function rewards true positive and is adversely affected by false negatives. A higher sensitivity is desirable here to minimize false negatives given the same accuracy score.

Tools to deal with imbalanced data

* Rebalancing:
   * Up-sample the fraud observations to reach 50/50
   * Down-sample the non-fraud to reach 50/50 (less recommended)
   * Use dollar-weighted probabilities
* Re-weighting:
   * Use auto-balancing
   * Apply a custom weight heuristic to the fitting function
* Adjusting specificity vs. sensitivity
   * Use ROC curve
   * Adjust threshold to minimize false negatives / maximize net revenue


#### Re-balancing

**Up-sampling**: We generate artificial data (or copies of existing data) based on the minor class observations. Artificial observations are generated until the ratio is improved or 50/50.

**Down-sampling**: We remove data from the major class observations until the ratio is improved or 50/50.

**Dollar-weighted probabilities**: Probabilities are scaled not by observations but also by the dollar amount, such that the number of observations for any record is scaled by the dollar-amount.

#### Re-weighting

Python's SKLearn can weight automatically based on the ratio of classes, or you can apply a custom weighting function to the individual classes you are fitting. Re-weighting can improve performance prior to using threshold adjustments in a ROC curve.

#### Adjusting the threshold between specificity and sensitivity

Increasing the sensitivity increases the true positive rate, but at the cost of increasing false-positives. Increasing the specificity improves the true negative rate, but at the cost of increasing false negatives and reduce true positive rate. These measures are in direct trade-off and can be adjusted by choosing the decision threshold.

**Example**: You are trying to classify accounts receivable at risk for delinquency. You start with a cut-off of 50%. If you increase the cut-off to 75%, you increase the true-positive rate, but you capture fewer positives (higher false negative rate). Similarly if you reduce the threshold to 25% you will capture more true positives, but you will have many false positives, reducing your true negative rate.

### Python and Sci-Kit Learn

Within the DecisionTree models in Sci-Kit learn, you can specify the following:

1. Adjust class weights with class_weight
2. Adjust information value metric with criterion {"gini", "entropy"}

See guidance at: [Sci-Kit Learn: Decision Tree Classifier Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier)

Confusion Matrices can be generated with the [sklearn.metrics](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html#sklearn.metrics.confusion_matrix) library.

Decision tree [examples](https://scikit-learn.org/stable/modules/tree.html#tree) on Sci-Kit Learn.
