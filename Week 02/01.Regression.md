# A brief introduction to machine learning

Linear regression, a supervised learning technique, is anchored in statistics; however, modern machine learning has shifted to take a slightly different approach, using validation performance to reduce the likelihood of over-fitting. Statistical tests for prob-values are then replaced with hyperparameter search methods to optimize model results.

As such, the Sci-Kit learn module we will use in Python does not necessarily provide the same statistic output we might be used to from statistics and in Excel.

These approaches started to merge towards one another when data size increased, with the use of training / validation or training / holdout sets to evaluate model performance (Using RMSPR or root mean squared prediction error and comparing it to the training model standard error). This concept was then expanded to cross-validation or k-fold out-of-sample testing.

Certain machine-learning techniques do not lend themselves to the same statistical rigor, especially when we consider topics such as Ridge Regression (regularization) and other techniques that have penalty functions in the solver.


### Video

[Video] - Placeholder


## Regression in Sci-Kit Learn

Sci-Kit learn offers many types of linear regression, with the most common being:
* Classical linear regression
* Ridge regression (Tikhonov or L<sub>2</sub> Regularization)
* Lasso regression (L<sub>1</sub> Regularization) 
* ElasticNet regression (L<sub>1</sub> and L<sub>2</sub> Regularization)

Regularization expands on classical regression by adding one or more hyperparameters that affects model performance. Hyperparameters are simply parameters that are selected prior to model construction. Choosing hyperparameters appropriately impacts the performance of the model.

#### Classical regression vs. Regularized regression

The focus of this course is not regularization; however, you are more likely than ever to encounter regularized regressions such as Ridge regression. 

Classical regression minimizes the errors from the following linear model:

<img src="https://render.githubusercontent.com/render/math?math=\mathbf{A}x = y">
<img src="https://render.githubusercontent.com/render/math?math=\min (e)^2 = \min (y - \mathbf{A}x)^2">

Regularization introduces a penalty function that shifts the minimization problem and makes it non-unique. We introduce a hyperparameter, λ, which controls the strength of the regularization and shifts the optimization problem to:

<img src="https://render.githubusercontent.com/render/math?math=\min (y - \mathbf{A}x)^2 + \lambda x^2">

The learning objective is not a deep understanding of the math, only to observe that regularization introduces a penalty function that is dependent on the coefficients, x. As lambda increases, regularization increases, and the coefficients are penalized. Stronger regularization prefers 'simpler models' or more specifically smaller norms of coefficients. In the limit that the lambda hyperparameter is very small and approaches zero, the Ridge regression regularization approaches classical regression.

#### Why regularize?
This may beg the question, why introduce an arbitrary penalty function to 'distort' the classical regression. Typically, regularization, when performed appropriately, outperforms classical regression because it encourages more general or robust models. While the model may not quite perform as well in a single training set as the classical regression model, well-built regularization models tend to out-perform classical regression with out-of-sample prediction.

Out-of-sample prediction is making predictions on data not used to train the model.

For this course, we will stick to classical regression due to small dataset size and to keep things simple.


#### Comparing SKLearn to traditional statistics
We will compare Sci-Kit Learn regression to traditional statistical output with the statsmodel library.


### Regression in Python
#### Libraries
```
import pandas as pd
from sklearn.linear_model import LinearRegression, Ridge
from sklearn import metrics
import statsmodels.api as sm
import datetime
```

Key Steps
1. Prepare your data for regression
2. Break your dataframe into an X and a Y dataset
3. Define the model with hyperparameters
4. Fit the model
5. Make predictions
6. Measure performance

Note that advanced machine learning users will often utilize a pipeline to increase the repeatability of the process, which is a bit beyond the scope of time we have for this additional, optional, supplemental content.

#### Fitting Linear Regression

```
LRmodel = LinearRegression()
model = LRmodel.fit(X, y)
```

#### Fitting Ridge Regression

```
RRmodel = Ridge(alpha = 1.0)  # Replace 1.0 w/ Lambda value
model = RRmodel.fit(X, y)
```

Note that alpha in the Sci-Kit Learn for Ridge Regression corresponds to lambda as described above (and common in machine-learning texts). Sometimes C is used for regularization coefficients, which is often an inverse of lambda.

#### Making Predictions
```
yp = model.predict(x)
```

#### Measuring Performance (Simple)
`
R2 = metrics.r2_score(y, yp)
Stderr = metrics.mean_squared_error(y, yp)**0.5
`

#### Getting coefficients
```
print(model.intercept_)
print(model.coef_)
```

#### Traditional Model Output using statsmodel
```
smodel = sm.OLS(y, X)
model = est.fit()
model.summary()
```

#### Data Preparation for Regression
Just like regression in Excel, data needs to be cleaned to be appropriately handled by the regression solvers. We introduce four techniques to manipulate data to prepare for regression:

    One-Hot encoding
    Date-Time adjustments
    Calculated Fields
    Lambda functions (advanced) (not the same lambda as regularization)


One-Hot Encoding

One-hot encoding maps categories to usable regression variables. Strict letter or string categories cannot run in a regression, but even numerical categories such as "day of week" should be encoded as there is not necessarily a meaningful relationship such that Wednesday is "2 more" than Monday.

One-Hot encoding is easy to do in Python with Pandas:

columns = pd.get_dummies(X['column_name'], drop_first=True)

The drop_first parameter says to leave one category out as the default (which is required for regression).

Example:

A dataframe, df, has a column called 'Day of the Week' where 1 = Monday, 2 = Tuesday, etc.

Original Data:
```
Row  Day of Week
1              1
2              4
3              2
4              1
...          ...
15             7
```

With One-Hot Encoding (Monday left out):
```
Row  Day of Week   T   W   H   F   S   U
1              1   0   0   0   0   0   0
2              4   0   0   1   0   0   0
3              2   1   0   0   0   0   0 
4              4   0   0   1   0   0   0
...          ...                     ...
15             7   0   0   0   0   0   1
```

#### Date Time Adjustments

Dates should not be entered in raw form to regression. The most common technique is to anchor or compute differences using datetime.timedelta.

For example, if we have a series of dates in a dataset, we might choose to convert these to days using:

```
df['Days'] = (df['Date'] - df['Date'].min()) / datetime.timedelta(days=1)
```

The numerator gets the difference in dates (in datetime format) and the division scales this to numbers of days, getting out of our datetime format and putting us in a numerical format that can be used for regression.

####Calculated Fields

Calculated fields in Python with Pandas is easy and logical. Strings and numbers can be combined with "+" such as:
```
df['Total Cost'] = df['Fixed Cost'] + df['Variable Cost']
```
where 'Total Cost' is a new column in the dataframe.

#### Lambda Functions

When more complex data manipulation is required, a lambda is a powerful tool to apply a one-time function to a column to adjust the data.

We use the .apply function to apply a function to a dataframe. While this can be a more generalized function defined in advance, lambdas tend to be clean and inline for purpose to ease understanding.


The form is:
```
df['New Column'] = df['column'].apply( lambda x: f(x) )
```
Where we replace f(x) with our inline function.

As an example to convert a single character code from 'a-z' to a number, we could use the following:
```
df['Row'] = df['Code'].apply( lambda x: ord(x) - ord('a') )
```
The power of lambda functions with .apply is that we can write exactly the function we need inline with our data manipulation.

The drawback of lambdas is that they are hard to understand for the novice programmer. Additionally, if you need to re-use the function, you should define the function in advance and use .apply with your function name.

Example of pre-defined function to convert strings of accounting numbers to a numerical format for analysis:
```
def clean_currency(x):
    """ Cleans strings of $ and () to numerical numbers that
    can be converted to a floating point number.
    Requires regular expression library 're'
    """
    if isinstance(x, str):
        return (re.sub('[(]', '-', re.sub( '[\$,)]', '', x)))
    return (x)

df['Amount'] = df['Currency'].apply(clean_currency).astype(float)
```
This code defines a function to use regular expressions to remove parenthesis and $ from a string while inserting a negative for negative numbers. We then apply this function to a currency string read in from a CSV file and convert the result to a numerical data type for analysis (float).


### Full Code Examples
1. Boston Housing Dataset:
   a. Compare classical linear regression and ridge regression
2. Duke Tickets Dataset:
   a. Prepare the Duke Tickets dataset for regression
   b. Compare classical linear regression and ridge regression


### References

#### FAQ

[Placeholder]

#### Links
* [SKLearn documentation: Linear Models](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.linear_model)
* [Géron - Chaptor 04 - Training Linear Models](https://github.com/ageron/handson-ml2/blob/master/04_training_linear_models.ipynb)
